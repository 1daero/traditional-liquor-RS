{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e23bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma, Komoran, Okt\n",
    "\n",
    "okt = Okt()\n",
    "kkm = Kkma()\n",
    "kom = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b640ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "kom = Komoran()\n",
    "\n",
    "def komorean_tokenizer(text):\n",
    "    tokens_ko = kom.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18792f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "def tokenizer(text):\n",
    "    # 내가 원하는 품사 (첫 알파벳)\n",
    "    my_tag = (\"N\", \"V\", \"M\", \"XR\")\n",
    "    # 형태소 분석기 정의\n",
    "    kkma = Kkma()\n",
    "    # 형태소 분석하기\n",
    "    words_with_tag = kkma.pos(text)\n",
    "    # 조건에 맞는 단어만 남겨놓기\n",
    "    words = [word for word, tag in words_with_tag if (len(word) > 1) and (tag.startswith(my_tag))]\n",
    "    \n",
    "    return words\n",
    "# Twiter 형태소 분석기 객체 생성\n",
    "twitter = Okt()\n",
    "\n",
    "# text를 형태소 분석해주는 함수\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = twitter.mor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb33d00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nl/n0qszn8x2xnfmwlkhfgf8pzc0000gn/T/ipykernel_6713/637508254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train , X_test , y_train , y_test = train_test_split(total_df['reviews'], total_df['label'],\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                        test_size=0.3,  random_state=11)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(total_df['reviews'], total_df['label'],\n",
    "                                                       test_size=0.3,  random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TfidfVectorizer 객체 생성\n",
    "tfidf_vect = TfidfVectorizer(tokenizer= komorean_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "# 피처 벡터화\n",
    "tfidf_vect.fit(X_train)\n",
    "tfidf_matrix_train = tfidf_vect.transform(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
